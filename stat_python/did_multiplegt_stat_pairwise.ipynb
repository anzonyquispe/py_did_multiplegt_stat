{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cb4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "did_multiplegt_stat_pairwise.py\n",
    "\n",
    "A faithful, functional Python translation (pandas + statsmodels) of the R internal\n",
    "function `did_multiplegt_stat_pairwise()`.\n",
    "\n",
    "Key design choices:\n",
    "- Keep the same logic/flow as the R code (pairwise DiD between consecutive periods).\n",
    "- Keep variable names with `_XX` suffix (matches upstream standardization).\n",
    "- Add detailed comments to make debugging/maintenance easy.\n",
    "\n",
    "Dependencies:\n",
    "    pip install pandas numpy statsmodels patsy\n",
    "\n",
    "IMPORTANT ASSUMPTIONS (same as R path):\n",
    "- `df` already contains standardized columns created upstream:\n",
    "    ID_XX, T_XX, Y_XX, D_XX, (optional) Z_XX,\n",
    "    tsfilled_XX (0/1 panel-fill flag),\n",
    "    weight_XX (obs weights, optional),\n",
    "    and if clustering is used: cluster_XX and weight_c_XX (cluster weights, optional).\n",
    "- This function is called for a given `pairwise` time index p.\n",
    "  It uses T_XX in {p-1,p} (or {p-2,p-1,p} when placebo=True),\n",
    "  then internally relabels time to 1..2 (or 1..3).\n",
    "\n",
    "Return format:\n",
    "    {\n",
    "      \"scalars\": updated_scalars_dict,\n",
    "      \"to_add\":  df_with_ID_and_influence_functions_and_aux_columns (or None)\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Small robust helpers (avoid crashes on empty/all-NaN groups)\n",
    "# ============================================================\n",
    "\n",
    "def _nanmin_or_nan(s: pd.Series) -> float:\n",
    "    \"\"\"Return nanmin(s) but if empty or all-NaN -> NaN.\"\"\"\n",
    "    arr = s.to_numpy(dtype=float)\n",
    "    if arr.size == 0 or np.all(np.isnan(arr)):\n",
    "        return float(\"nan\")\n",
    "    return float(np.nanmin(arr))\n",
    "\n",
    "def _nanmax_or_nan(s: pd.Series) -> float:\n",
    "    \"\"\"Return nanmax(s) but if empty or all-NaN -> NaN.\"\"\"\n",
    "    arr = s.to_numpy(dtype=float)\n",
    "    if arr.size == 0 or np.all(np.isnan(arr)):\n",
    "        return float(\"nan\")\n",
    "    return float(np.nanmax(arr))\n",
    "\n",
    "\n",
    "\n",
    "def _nanmax_or_minus_inf(s: pd.Series) -> float:\n",
    "    \"\"\"Return nanmax(s) but if empty or all-NaN -> -Inf (matches R max(..., na.rm=TRUE)).\"\"\"\n",
    "    arr = s.to_numpy(dtype=float)\n",
    "    if arr.size == 0 or np.all(np.isnan(arr)):\n",
    "        return float(\"-inf\")\n",
    "    return float(np.nanmax(arr))\n",
    "\n",
    "def _ensure_numeric(df: pd.DataFrame, col: str) -> None:\n",
    "    \"\"\"Force a column to numeric in-place (coerce errors to NaN).\"\"\"\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Weighted utilities (mirroring your R utils.R semantics)\n",
    "# ============================================================\n",
    "\n",
    "def _get_weight_col(df: pd.DataFrame, w: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Return weight column name if it exists, else None.\"\"\"\n",
    "    if w is None:\n",
    "        w = \"weight_XX\"\n",
    "    return w if (w in df.columns) else None\n",
    "\n",
    "def wSum(df: pd.DataFrame, w: Optional[str] = None) -> float:\n",
    "    \"\"\"\n",
    "    Weighted sum of weights (NOT sum of a variable).\n",
    "    Used as \"effective N\" denominator in sd/sqrt(wSum).\n",
    "    \"\"\"\n",
    "    wcol = _get_weight_col(df, w)\n",
    "    if wcol is None:\n",
    "        return float(len(df))\n",
    "    _ensure_numeric(df, wcol)\n",
    "    return float(np.nansum(df[wcol].to_numpy(dtype=float)))\n",
    "\n",
    "def Mean(var: str, df: pd.DataFrame, w: Optional[str] = None) -> float:\n",
    "    \"\"\"Weighted mean of df[var] using df[w].\"\"\"\n",
    "    if var not in df.columns:\n",
    "        return float(\"nan\")\n",
    "    wcol = _get_weight_col(df, w)\n",
    "    _ensure_numeric(df, var)\n",
    "    x = df[var].to_numpy(dtype=float)\n",
    "\n",
    "    if wcol is None:\n",
    "        return float(np.nanmean(x)) if np.any(~np.isnan(x)) else float(\"nan\")\n",
    "\n",
    "    _ensure_numeric(df, wcol)\n",
    "    ww = df[wcol].to_numpy(dtype=float)\n",
    "    mask = (~np.isnan(x)) & (~np.isnan(ww))\n",
    "    if not np.any(mask):\n",
    "        return float(\"nan\")\n",
    "    return float(np.average(x[mask], weights=ww[mask]))\n",
    "\n",
    "def Sum(var: str, df: pd.DataFrame, w: Optional[str] = None) -> float:\n",
    "    \"\"\"Weighted sum Σ w_i * x_i of df[var].\"\"\"\n",
    "    if var not in df.columns:\n",
    "        return 0.0\n",
    "    wcol = _get_weight_col(df, w)\n",
    "    _ensure_numeric(df, var)\n",
    "    x = df[var].to_numpy(dtype=float)\n",
    "\n",
    "    if wcol is None:\n",
    "        return float(np.nansum(x))\n",
    "\n",
    "    _ensure_numeric(df, wcol)\n",
    "    ww = df[wcol].to_numpy(dtype=float)\n",
    "    mask = (~np.isnan(x)) & (~np.isnan(ww))\n",
    "    return float(np.sum(x[mask] * ww[mask]))\n",
    "\n",
    "def Sd(var: str, df: pd.DataFrame, w: Optional[str] = None) -> float:\n",
    "    \"\"\"\n",
    "    Weighted standard deviation mirroring R's Hmisc::wtd.var default:\n",
    "        Sd(x,w) = sqrt( Σ w_i (x_i-μ_w)^2 / (Σ w_i - 1) )\n",
    "\n",
    "    Notes:\n",
    "    - This corresponds to frequency weights with an \"unbiased\" denominator (sum(w)-1).\n",
    "    - If no weights are provided/available, fall back to the usual sample sd (ddof=1).\n",
    "    \"\"\"\n",
    "    if var not in df.columns:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    wcol = _get_weight_col(df, w)\n",
    "    _ensure_numeric(df, var)\n",
    "    x = df[var].to_numpy(dtype=float)\n",
    "\n",
    "    if wcol is None:\n",
    "        return float(np.nanstd(x, ddof=1))\n",
    "\n",
    "    _ensure_numeric(df, wcol)\n",
    "    ww = df[wcol].to_numpy(dtype=float)\n",
    "    mask = (~np.isnan(x)) & (~np.isnan(ww))\n",
    "    if not np.any(mask):\n",
    "        return float(\"nan\")\n",
    "\n",
    "    x = x[mask]\n",
    "    ww = ww[mask]\n",
    "    sw = float(np.sum(ww))\n",
    "    if sw <= 1.0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    mu = float(np.sum(ww * x) / sw)\n",
    "    varw = float(np.sum(ww * (x - mu) ** 2) / (sw - 1.0))\n",
    "    return float(np.sqrt(varw))\n",
    "\n",
    "# ============================================================\n",
    "# Statsmodels wrappers to mimic the R \"stata_logit\" + predict\n",
    "# ============================================================\n",
    "\n",
    "def stata_logit(formula: str, df: pd.DataFrame, wcol: str = \"weight_XX\",\n",
    "                maxit: int = 300, tol: float = 1e-8):\n",
    "    \"\"\"\n",
    "    Emulate R `stata_logit()`:\n",
    "    Fit weighted logit using GLM Binomial with frequency weights.\n",
    "    \"\"\"\n",
    "    if wcol in df.columns:\n",
    "        _ensure_numeric(df, wcol)\n",
    "        freq_w = df[wcol]\n",
    "        freq_w = freq_w.fillna(0.0)\n",
    "    else:\n",
    "        freq_w = None\n",
    "\n",
    "    model = smf.glm(\n",
    "        formula=formula,\n",
    "        data=df,\n",
    "        family=sm.families.Binomial(),\n",
    "        freq_weights=freq_w\n",
    "    )\n",
    "    res = model.fit(maxiter=maxit, tol=tol, disp=0)\n",
    "    return res\n",
    "\n",
    "def _lpredict_fallback(df: pd.DataFrame, outcol: str, fitted_model) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add predictions to df[outcol].\n",
    "\n",
    "    This mirrors the R helper `lpredict()` used in the original package:\n",
    "    - For Binomial GLM (logit): predicted probabilities, then apply:\n",
    "        * NaN -> 1\n",
    "        * values < 1e-10 -> 0\n",
    "      (R uses `sensitivity <- 10^-10` and applies these only for prob=TRUE.)\n",
    "    - For WLS/OLS: fitted values (no clipping).\n",
    "\n",
    "    Notes:\n",
    "    - This behavior matters for strict replication of the WAOSS/IVWAOSS paths where\n",
    "      propensity scores close to zero are hard-trimmed in R.\n",
    "    \"\"\"\n",
    "    pred = np.asarray(fitted_model.predict(df), dtype=float)\n",
    "\n",
    "    # Detect Binomial GLM results (statsmodels)\n",
    "    is_binom = False\n",
    "    try:\n",
    "        fam = getattr(getattr(fitted_model, \"model\", None), \"family\", None)\n",
    "        is_binom = isinstance(fam, sm.families.Binomial)\n",
    "    except Exception:\n",
    "        is_binom = False\n",
    "\n",
    "    if is_binom:\n",
    "        sensitivity = 1e-10\n",
    "        pred = np.where(np.isnan(pred), 1.0, pred)\n",
    "        pred = np.where(pred < sensitivity, 0.0, pred)\n",
    "\n",
    "    df[outcol] = pred\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Compatibility wrapper for `lpredict`\n",
    "# ============================================================\n",
    "# In this project, `lpredict` is implemented in `lpredict.ipynb` with the same\n",
    "# signature as the R helper:\n",
    "#   _lpredict(df, varname, model, varlist, const=True, prob=False, factor=False)\n",
    "#\n",
    "# Earlier versions of this file used a simplified internal `_lpredict(df, outcol, model)`.\n",
    "# To make the code robust (and to avoid Jupyter order-of-execution issues), we:\n",
    "#   - keep a fallback implementation (`_lpredict_fallback`) that works with statsmodels\n",
    "#   - call the external `lpredict` if it is already loaded AND has the R-like signature\n",
    "#   - otherwise, fall back to `_lpredict_fallback`.\n",
    "#\n",
    "# IMPORTANT: For logit/GLM(Binomial) models, R calls lpredict(..., prob=TRUE).\n",
    "# The wrapper below auto-detects Binomial models and forces `prob=True` in that case.\n",
    "\n",
    "import inspect\n",
    "\n",
    "def _infer_varlist_from_model(model) -> list[str]:\n",
    "    \"\"\"Infer RHS variable names from a statsmodels-like fitted model.\"\"\"\n",
    "    if hasattr(model, \"params\"):\n",
    "        params = model.params\n",
    "        try:\n",
    "            names = list(params.index)\n",
    "        except Exception:\n",
    "            names = []\n",
    "    else:\n",
    "        names = []\n",
    "    drop = {\"(Intercept)\", \"Intercept\", \"const\"}\n",
    "    return [n for n in names if n not in drop]\n",
    "\n",
    "def _is_binomial_glm(model) -> bool:\n",
    "    try:\n",
    "        fam = getattr(getattr(model, \"model\", None), \"family\", None)\n",
    "        return isinstance(fam, sm.families.Binomial)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _lpredict(df: pd.DataFrame, varname: str, model, prob: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Predict into df[varname], using external R-like lpredict if available.\"\"\"\n",
    "    prob_eff = bool(prob) or _is_binomial_glm(model)\n",
    "\n",
    "    ext = globals().get(\"lpredict\", None)\n",
    "    if callable(ext):\n",
    "        try:\n",
    "            sig = inspect.signature(ext)\n",
    "            if len(sig.parameters) >= 4:\n",
    "                varlist = _infer_varlist_from_model(model)\n",
    "                return ext(df, varname, model, varlist, const=True, prob=prob_eff, factor=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return _lpredict_fallback(df, varname, model)\n",
    "\n",
    "\n",
    "# Alias used by upstream calls in this module\n",
    "__lpredict = _lpredict\n",
    "\n",
    "# ============================================================\n",
    "# Main: did_multiplegt_stat_pairwise\n",
    "# ============================================================\n",
    "\n",
    "def did_multiplegt_stat_pairwise(\n",
    "    df: pd.DataFrame,\n",
    "    Y: str,\n",
    "    ID: str,\n",
    "    Time: str,\n",
    "    D: str,\n",
    "    Z: Optional[str],\n",
    "    estimator: str,\n",
    "    order: int,\n",
    "    noextrapolation: bool,\n",
    "    weight: Optional[str],\n",
    "    switchers: Optional[str],\n",
    "    pairwise: int,\n",
    "    IDs: Any,\n",
    "    aoss: int,\n",
    "    waoss: int,\n",
    "    ivwaoss: int,\n",
    "    estimation_method: str,\n",
    "    scalars: Dict[str, Any],\n",
    "    placebo: int,  # FIXED: 0=no placebo, 1+=placebo index\n",
    "    exact_match: bool,\n",
    "    cluster: Optional[str],\n",
    "    by_fd_opt: Optional[int],\n",
    "    other_treatments: Optional[List[str]],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Internal function for estimation of pairwise DiD between consecutive time periods.\n",
    "\n",
    "    NOTE:\n",
    "    - Interface keeps Y/ID/Time/D/Z for compatibility, but function uses *_XX columns.\n",
    "    - `scalars` is updated in-place (like in R), and returned too.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 0) Ensure standardized *_XX columns exist (standalone-friendly)\n",
    "    # ------------------------------------------------------------\n",
    "    if \"ID_XX\" not in df.columns and ID in df.columns:\n",
    "        df[\"ID_XX\"] = df[ID]\n",
    "    if \"T_XX\" not in df.columns and Time in df.columns:\n",
    "        df[\"T_XX\"] = df[Time]\n",
    "    if \"Y_XX\" not in df.columns and Y in df.columns:\n",
    "        df[\"Y_XX\"] = df[Y]\n",
    "    if \"D_XX\" not in df.columns and D in df.columns:\n",
    "        df[\"D_XX\"] = df[D]\n",
    "    if Z is not None:\n",
    "        if \"Z_XX\" not in df.columns and Z in df.columns:\n",
    "            df[\"Z_XX\"] = df[Z]\n",
    "    # `tsfilled_XX` is 0/1 indicating panel-filled rows (created upstream in R main).\n",
    "    # If absent, default to 0 (assume original observations).\n",
    "    if \"tsfilled_XX\" not in df.columns:\n",
    "        df[\"tsfilled_XX\"] = 0.0\n",
    "    # Observation weights: in the R code, missing weights are set to 0 (not dropped).\n",
    "    if weight is not None and (\"weight_XX\" not in df.columns) and (weight in df.columns):\n",
    "        df[\"weight_XX\"] = df[weight]\n",
    "    # Cluster id alias (if provided and not already standardized)\n",
    "    if cluster is not None and (\"cluster_XX\" not in df.columns) and (cluster in df.columns):\n",
    "        df[\"cluster_XX\"] = df[cluster]\n",
    "\n",
    "    pl = \"_pl\" if placebo > 0 else \"\"\n",
    "    placebo_index = placebo  # FIXED: store placebo index\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1) Subset time window: {p-1,p} or {p-2,p-1,p} for placebo\n",
    "    # ------------------------------------------------------------\n",
    "    # FIXED: Period selection based on placebo index\n",
    "    if placebo_index == 0:\n",
    "        # Main effect: periods {p-1, p}\n",
    "        df = df[df[\"T_XX\"].isin([pairwise - 1, pairwise])]\n",
    "    else:\n",
    "        # Placebo: periods {p - placebo - 1, p - placebo, p - 1, p}\n",
    "        periods_to_keep = sorted(set([\n",
    "            pairwise - placebo_index - 1,\n",
    "            pairwise - placebo_index,\n",
    "            pairwise - 1,\n",
    "            pairwise\n",
    "        ]))\n",
    "        df = df[df[\"T_XX\"].isin(periods_to_keep)]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2) Detect \"gap\" via tsfilled_XX\n",
    "    #    gap_XX = max_t min_i tsfilled_{it}\n",
    "    # ------------------------------------------------------------\n",
    "    if len(df) == 0:\n",
    "        gap_XX = 1.0\n",
    "    else:\n",
    "        df[\"tsfilled_min_XX\"] = df.groupby(\"T_XX\")[\"tsfilled_XX\"].transform(_nanmin_or_nan)\n",
    "        gap_XX = float(_nanmax_or_nan(df[\"tsfilled_min_XX\"]))\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3) Relabel time to consecutive ids: 1..k\n",
    "    # ------------------------------------------------------------\n",
    "    if len(df) > 0:\n",
    "        tvals = np.sort(df[\"T_XX\"].dropna().unique())\n",
    "        tmap = {t: i + 1 for i, t in enumerate(tvals)}\n",
    "        df[\"T_XX\"] = df[\"T_XX\"].map(tmap).astype(float)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4) Sort and compute within-ID first differences\n",
    "    # ------------------------------------------------------------\n",
    "    _ensure_numeric(df, \"ID_XX\")\n",
    "    df = df.sort_values([\"ID_XX\", \"T_XX\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    g = df.groupby(\"ID_XX\")\n",
    "    df[\"delta_Y_XX\"] = g[\"Y_XX\"].diff()  # backward diff: Y_t - Y_{t-1} (plm::diff alignment)\n",
    "    df[\"delta_D_XX\"] = g[\"D_XX\"].diff()  # backward diff: D_t - D_{t-1} (plm::diff alignment)\n",
    "\n",
    "    if ivwaoss == 1:\n",
    "        if \"Z_XX\" not in df.columns:\n",
    "            raise ValueError(\"ivwaoss==1 but df has no Z_XX column.\")\n",
    "        df[\"delta_Z_XX\"] = g[\"Z_XX\"].diff()  # backward diff: Z_t - Z_{t-1} (plm::diff alignment)\n",
    "\n",
    "    # Other treatments: fd_ot = sum(diff(ot)) within ID\n",
    "    if other_treatments:\n",
    "        for v in other_treatments:\n",
    "            df[f\"_fdtmp_{v}\"] = df.groupby(\"ID_XX\")[v].diff()\n",
    "        for v in other_treatments:\n",
    "            df[f\"fd_{v}_XX\"] = df.groupby(\"ID_XX\")[f\"_fdtmp_{v}\"].transform(\n",
    "                lambda s: float(np.nansum(s.to_numpy(dtype=float)))\n",
    "            )\n",
    "            df.drop(columns=[f\"_fdtmp_{v}\"], inplace=True)\n",
    "\n",
    "    # by_fd: keep lead of partition then drop partition\n",
    "    if \"partition_XX\" in df.columns:\n",
    "        df[\"partition_lead_XX\"] = df.groupby(\"ID_XX\")[\"partition_XX\"].shift(-1)\n",
    "        df.drop(columns=[\"partition_XX\"], inplace=True)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 5) Make delta_Y constant per ID\n",
    "    # ------------------------------------------------------------\n",
    "    # FIXED: deltaY calculation based on placebo index\n",
    "    if placebo_index == 0:\n",
    "        df[\"delta_Y_XX\"] = df.groupby(\"ID_XX\")[\"delta_Y_XX\"].transform(\"mean\")\n",
    "    else:\n",
    "        # For placebo: take deltaY at T_XX == 2 (corresponds to Y_{t-placebo} - Y_{t-placebo-1})\n",
    "        df[\"delta_temp\"] = np.where(df[\"T_XX\"] == 2, df[\"delta_Y_XX\"], np.nan)\n",
    "        df[\"delta_Y_XX\"] = df.groupby(\"ID_XX\")[\"delta_temp\"].transform(\"mean\")\n",
    "        df.drop(columns=[\"delta_temp\"], inplace=True)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 6) Placebo restriction\n",
    "    # ------------------------------------------------------------\n",
    "    # FIXED: Placebo restriction based on placebo index\n",
    "    if placebo_index > 0 and (aoss == 1 or waoss == 1):\n",
    "        df[\"inSamplePlacebo_temp_XX\"] = np.where(\n",
    "            (df[\"delta_D_XX\"] == 0) & (df[\"T_XX\"] == 2),\n",
    "            1.0,\n",
    "            0.0,\n",
    "        )\n",
    "        df.loc[df[\"delta_D_XX\"].isna(), \"inSamplePlacebo_temp_XX\"] = np.nan\n",
    "        df[\"inSamplePlacebo_XX\"] = df.groupby(\"ID_XX\")[\"inSamplePlacebo_temp_XX\"].transform(_nanmax_or_minus_inf)\n",
    "\n",
    "        # Stata: drop if T_XX == 1\n",
    "        df = df[df[\"T_XX\"] != 1]\n",
    "        \n",
    "        # Stata: if placebo > 1: drop if T_XX == 2\n",
    "        if placebo_index > 1:\n",
    "            df = df[df[\"T_XX\"] != 2]\n",
    "        \n",
    "        # Stata: deltaD only at the correct period\n",
    "        # if placebo == 1: deltaD = . if T_XX != 3\n",
    "        # if placebo > 1: deltaD = . if T_XX != 4\n",
    "        if placebo_index == 1:\n",
    "            df[\"delta_D_XX\"] = np.where(df[\"T_XX\"] != 3, np.nan, df[\"delta_D_XX\"])\n",
    "        else:  # placebo_index > 1\n",
    "            df[\"delta_D_XX\"] = np.where(df[\"T_XX\"] != 4, np.nan, df[\"delta_D_XX\"])\n",
    "\n",
    "    # FIXED: IV placebo restriction based on placebo index\n",
    "    if placebo_index > 0 and ivwaoss == 1:\n",
    "        df[\"inSamplePlacebo_IV_temp_XX\"] = np.where(\n",
    "            (df[\"delta_Z_XX\"] == 0) & (df[\"T_XX\"] == 2),\n",
    "            1.0,\n",
    "            0.0,\n",
    "        )\n",
    "        df.loc[df[\"delta_Z_XX\"].isna(), \"inSamplePlacebo_IV_temp_XX\"] = np.nan\n",
    "        df[\"inSamplePlacebo_XX\"] = df.groupby(\"ID_XX\")[\"inSamplePlacebo_IV_temp_XX\"].transform(_nanmax_or_minus_inf)\n",
    "\n",
    "        df = df[df[\"T_XX\"] != 1]\n",
    "        \n",
    "        if placebo_index > 1:\n",
    "            df = df[df[\"T_XX\"] != 2]\n",
    "        \n",
    "        if placebo_index == 1:\n",
    "            df[\"delta_Z_XX\"] = np.where(df[\"T_XX\"] != 3, np.nan, df[\"delta_Z_XX\"])\n",
    "        else:\n",
    "            df[\"delta_Z_XX\"] = np.where(df[\"T_XX\"] != 4, np.nan, df[\"delta_Z_XX\"])\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 7) Empty after restrictions => early exit\n",
    "    # ------------------------------------------------------------\n",
    "    if len(df) == 0:\n",
    "        if aoss == 1:\n",
    "            scalars[f\"P_{pairwise}{pl}_XX\"] = 0.0\n",
    "        if waoss == 1:\n",
    "            scalars[f\"E_abs_delta_D_{pairwise}{pl}_XX\"] = 0.0\n",
    "        if ivwaoss == 1:\n",
    "            scalars[f\"denom_delta_IV_{pairwise}{pl}_XX\"] = 0.0\n",
    "\n",
    "        scalars[f\"non_missing_{pairwise}{pl}_XX\"] = 0.0\n",
    "\n",
    "        for v in (\"Switchers\", \"Stayers\"):\n",
    "            for n in (1, 2, 3):\n",
    "                scalars[f\"N_{v}_{n}_{pairwise}{pl}_XX\"] = 0.0\n",
    "\n",
    "        for i, active in enumerate([aoss, waoss, ivwaoss], start=1):\n",
    "            if active == 1:\n",
    "                scalars[f\"delta_{i}_{pairwise}{pl}_XX\"] = 0.0\n",
    "                scalars[f\"sd_delta_{i}_{pairwise}{pl}_XX\"] = np.nan\n",
    "                scalars[f\"LB_{i}_{pairwise}{pl}_XX\"] = np.nan\n",
    "                scalars[f\"UB_{i}_{pairwise}{pl}_XX\"] = np.nan\n",
    "\n",
    "        return {\"scalars\": scalars, \"to_add\": None}\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 8) Make delta_D (and delta_Z) constant per ID\n",
    "    # ------------------------------------------------------------\n",
    "    df[\"delta_D_XX\"] = df.groupby(\"ID_XX\")[\"delta_D_XX\"].transform(\"mean\")\n",
    "\n",
    "    if ivwaoss == 1:\n",
    "        df[\"delta_Z_XX\"] = df.groupby(\"ID_XX\")[\"delta_Z_XX\"].transform(\"mean\")\n",
    "        df[\"SI_XX\"] = np.sign(df[\"delta_Z_XX\"]).astype(float)\n",
    "        df[\"Z1_XX\"] = df[\"Z_XX\"]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 9) used_in indicators + switcher sign S_XX + abs deltas\n",
    "    # ------------------------------------------------------------\n",
    "    df[f\"used_in_{pairwise}{pl}_XX\"] = (\n",
    "        (~df[\"delta_Y_XX\"].isna()) & (~df[\"delta_D_XX\"].isna())\n",
    "    ).astype(float)\n",
    "\n",
    "    if ivwaoss == 1:\n",
    "        df[f\"used_in_IV_{pairwise}{pl}_XX\"] = (\n",
    "            (df[f\"used_in_{pairwise}{pl}_XX\"] == 1.0) & (~df[\"delta_Z_XX\"].isna())\n",
    "        ).astype(float)\n",
    "        df = df[df[f\"used_in_IV_{pairwise}{pl}_XX\"] == 1.0]\n",
    "\n",
    "    df[\"S_XX\"] = np.sign(df[\"delta_D_XX\"]).astype(float)\n",
    "\n",
    "    if (waoss == 1 or aoss == 1):\n",
    "        df[\"abs_delta_D_XX\"] = df[\"S_XX\"] * df[\"delta_D_XX\"]\n",
    "        if switchers == \"up\":\n",
    "            df = df[df[\"S_XX\"] != -1.0]\n",
    "        elif switchers == \"down\":\n",
    "            df = df[df[\"S_XX\"] != 1.0]\n",
    "\n",
    "    if ivwaoss == 1:\n",
    "        if switchers == \"up\":\n",
    "            df = df[df[\"SI_XX\"] != -1.0]\n",
    "        elif switchers == \"down\":\n",
    "            df = df[df[\"SI_XX\"] != 1.0]\n",
    "        df[\"abs_delta_Z_XX\"] = df[\"SI_XX\"] * df[\"delta_Z_XX\"]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 10) Drop the second-year line (keep first row of pair)\n",
    "    # ------------------------------------------------------------\n",
    "    df = df[df[\"T_XX\"] != df[\"T_XX\"].max()]\n",
    "\n",
    "    df[\"D1_XX\"] = df[\"D_XX\"]\n",
    "    df.drop(columns=[\"D_XX\"], inplace=True)\n",
    "\n",
    "    df[\"Ht_XX\"] = ((~df[\"delta_D_XX\"].isna()) & (~df[\"delta_Y_XX\"].isna())).astype(float)\n",
    "    df.loc[df[\"Ht_XX\"] == 0, \"S_XX\"] = np.nan\n",
    "\n",
    "    if ivwaoss == 1:\n",
    "        df[\"Ht_XX\"] = ((df[\"Ht_XX\"] == 1.0) & (~df[\"delta_Z_XX\"].isna())).astype(float)\n",
    "        df.loc[df[\"Ht_XX\"] == 0, \"SI_XX\"] = np.nan\n",
    "\n",
    "    if by_fd_opt is not None and \"partition_lead_XX\" in df.columns:\n",
    "        df = df[(df[\"partition_lead_XX\"] == 0) | (df[\"partition_lead_XX\"] == by_fd_opt)]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 11) Set missing if placebo condition fails or other_treatments change\n",
    "    # ------------------------------------------------------------\n",
    "    vars_to_set_missing = [\"S_XX\", \"delta_D_XX\", \"delta_Y_XX\", \"D1_XX\"]\n",
    "    if aoss == 1 or waoss == 1:\n",
    "        vars_to_set_missing += [\"abs_delta_D_XX\"]\n",
    "    else:\n",
    "        vars_to_set_missing += [\"Z1_XX\", \"SI_XX\"]\n",
    "\n",
    "    if placebo_index > 0 and \"inSamplePlacebo_XX\" in df.columns:\n",
    "        mask_bad = (df[\"inSamplePlacebo_XX\"] == 0)\n",
    "        for v in vars_to_set_missing:\n",
    "            if v in df.columns:\n",
    "                df.loc[mask_bad, v] = np.nan\n",
    "        df.loc[mask_bad, \"Ht_XX\"] = np.nan\n",
    "\n",
    "    if other_treatments:\n",
    "        for ot in other_treatments:\n",
    "            colfd = f\"fd_{ot}_XX\"\n",
    "            if colfd in df.columns:\n",
    "                mask_bad = (df[colfd] != 0)\n",
    "                for v in vars_to_set_missing:\n",
    "                    if v in df.columns:\n",
    "                        df.loc[mask_bad, v] = np.nan\n",
    "                df.loc[mask_bad, \"Ht_XX\"] = np.nan\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 12) No-extrapolation trimming\n",
    "    # ------------------------------------------------------------\n",
    "    scalars.setdefault(\"N_drop_total_XX\", 0.0)\n",
    "    scalars.setdefault(\"N_drop_total_C_XX\", 0.0)\n",
    "\n",
    "    if noextrapolation:\n",
    "        if aoss == 1 or waoss == 1:\n",
    "            stayers = df[df[\"S_XX\"] == 0]\n",
    "            if len(stayers):\n",
    "                max_D1 = float(np.nanmax(stayers[\"D1_XX\"]))\n",
    "                min_D1 = float(np.nanmin(stayers[\"D1_XX\"]))\n",
    "            else:\n",
    "                # mimic R's max/min with na.rm=TRUE on empty: -Inf / +Inf\n",
    "                max_D1 = float(\"-inf\")\n",
    "                min_D1 = float(\"inf\")\n",
    "\n",
    "            d1 = df[\"D1_XX\"].to_numpy(dtype=float)\n",
    "            df[\"outofBounds_XX\"] = (~np.isnan(d1)) & ((d1 < min_D1) | (d1 > max_D1))\n",
    "            N_drop = float(np.nansum(df[\"outofBounds_XX\"].astype(float)))\n",
    "            scalars[f\"N_drop_{pairwise}{pl}_XX\"] = N_drop\n",
    "            df = df[~df[\"outofBounds_XX\"]]\n",
    "\n",
    "            if (N_drop > 0) and (placebo_index == 0) and (gap_XX == 0) and (N_drop < len(df) - 1):\n",
    "                scalars[\"N_drop_total_XX\"] += N_drop\n",
    "\n",
    "        if ivwaoss == 1:\n",
    "            stayers = df[df[\"SI_XX\"] == 0]\n",
    "            if len(stayers):\n",
    "                max_Z1 = float(np.nanmax(stayers[\"Z1_XX\"]))\n",
    "                min_Z1 = float(np.nanmin(stayers[\"Z1_XX\"]))\n",
    "            else:\n",
    "                # mimic R's max/min with na.rm=TRUE on empty: -Inf / +Inf\n",
    "                max_Z1 = float(\"-inf\")\n",
    "                min_Z1 = float(\"inf\")\n",
    "\n",
    "            z1 = df[\"Z1_XX\"].to_numpy(dtype=float)\n",
    "            df[\"outofBoundsIV_XX\"] = (~np.isnan(z1)) & ((z1 < min_Z1) | (z1 > max_Z1))\n",
    "            N_IVdrop = float(np.nansum(df[\"outofBoundsIV_XX\"].astype(float)))\n",
    "            scalars[f\"N_IVdrop_{pairwise}{pl}_XX\"] = N_IVdrop\n",
    "            df = df[~df[\"outofBoundsIV_XX\"]]\n",
    "\n",
    "            if (N_IVdrop > 0) and (placebo_index == 0) and (gap_XX == 0) and (N_IVdrop < len(df) - 1):\n",
    "                scalars[\"N_drop_total_XX\"] += N_IVdrop\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 13) Exact matching feasibility drops\n",
    "    # ------------------------------------------------------------\n",
    "    if exact_match:\n",
    "        if aoss == 1 or waoss == 1:\n",
    "            group_cols = [\"D1_XX\"] + (other_treatments or [])\n",
    "            g = df.groupby(group_cols, dropna=False)\n",
    "\n",
    "            df[\"has_match_min_XX\"] = g[\"abs_delta_D_XX\"].transform(_nanmin_or_nan)\n",
    "            df[\"has_match_max_XX\"] = g[\"abs_delta_D_XX\"].transform(_nanmax_or_minus_inf)\n",
    "\n",
    "            df[\"s_has_match_XX\"] = np.where(\n",
    "                ~df[\"S_XX\"].isna(),\n",
    "                (df[\"has_match_min_XX\"] == 0).astype(float),\n",
    "                -1.0\n",
    "            )\n",
    "            df.loc[df[\"S_XX\"] == 0, \"s_has_match_XX\"] = -1.0\n",
    "\n",
    "            df[\"c_has_match_XX\"] = np.where(\n",
    "                ~df[\"S_XX\"].isna(),\n",
    "                (df[\"has_match_max_XX\"] > 0).astype(float),\n",
    "                -1.0\n",
    "            )\n",
    "            df.loc[(df[\"S_XX\"] != 0) & (~df[\"S_XX\"].isna()), \"c_has_match_XX\"] = -1.0\n",
    "\n",
    "        else:\n",
    "            group_cols = [\"Z1_XX\"] + (other_treatments or [])\n",
    "            g = df.groupby(group_cols, dropna=False)\n",
    "\n",
    "            df[\"has_match_min_XX\"] = g[\"abs_delta_Z_XX\"].transform(_nanmin_or_nan)\n",
    "            df[\"has_match_max_XX\"] = g[\"abs_delta_Z_XX\"].transform(_nanmax_or_minus_inf)\n",
    "\n",
    "            df[\"s_has_match_XX\"] = np.where(\n",
    "                ~df[\"SI_XX\"].isna(),\n",
    "                (df[\"has_match_min_XX\"] == 0).astype(float),\n",
    "                -1.0\n",
    "            )\n",
    "            df.loc[df[\"SI_XX\"] == 0, \"s_has_match_XX\"] = -1.0\n",
    "\n",
    "            df[\"c_has_match_XX\"] = np.where(\n",
    "                ~df[\"SI_XX\"].isna(),\n",
    "                (df[\"has_match_max_XX\"] > 0).astype(float),\n",
    "                -1.0\n",
    "            )\n",
    "            df.loc[(df[\"SI_XX\"] != 0) & (~df[\"SI_XX\"].isna()), \"c_has_match_XX\"] = -1.0\n",
    "\n",
    "        N_drop_s = float((df[\"s_has_match_XX\"] == 0).sum())\n",
    "        N_drop_c = float((df[\"c_has_match_XX\"] == 0).sum())\n",
    "        scalars[f\"N_drop_{pairwise}{pl}_XX\"] = N_drop_s\n",
    "        scalars[f\"N_drop_{pairwise}{pl}_C_XX\"] = N_drop_c\n",
    "\n",
    "        if (N_drop_s > 0) and (N_drop_s != len(df)) and (gap_XX == 0):\n",
    "            scalars[\"N_drop_total_XX\"] += N_drop_s\n",
    "        if (N_drop_c > 0) and (N_drop_c != len(df)) and (gap_XX == 0):\n",
    "            scalars[\"N_drop_total_C_XX\"] += N_drop_c\n",
    "\n",
    "        mask_bad = (df[\"s_has_match_XX\"] == 0) | (df[\"c_has_match_XX\"] == 0)\n",
    "        for v in vars_to_set_missing:\n",
    "            if v in df.columns:\n",
    "                df.loc[mask_bad, v] = np.nan\n",
    "        df.loc[mask_bad, \"Ht_XX\"] = np.nan\n",
    "\n",
    "        if \"D1_XX\" in df.columns:\n",
    "            nun = int(df[\"D1_XX\"].nunique(dropna=True))\n",
    "            if nun >= 1:\n",
    "                order = min(order, nun)\n",
    "\n",
    "        df.drop(columns=[c for c in [\"has_match_min_XX\", \"has_match_max_XX\"] if c in df.columns], inplace=True)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 14) Bookkeeping scalars for this pair\n",
    "    # ------------------------------------------------------------\n",
    "    if \"weight_XX\" not in df.columns:\n",
    "        df[\"weight_XX\"] = 1.0\n",
    "    _ensure_numeric(df, \"weight_XX\")\n",
    "\n",
    "    # R behavior: set missing weights to 0 (not NA)\n",
    "    df[\"weight_XX\"] = df[\"weight_XX\"].fillna(0.0)\n",
    "    # Guardrail: negative weights are almost surely unintended\n",
    "    if (df[\"weight_XX\"] < 0).any():\n",
    "        raise ValueError(\"Negative weights are not supported (weight_XX < 0).\")\n",
    "\n",
    "    scalars[f\"W{pl}_XX\"] = float(np.nansum(df[\"weight_XX\"].to_numpy(dtype=float)))\n",
    "    scalars[f\"N{pl}_XX\"] = float(len(df))\n",
    "\n",
    "    if waoss == 1 or aoss == 1:\n",
    "        scalars[f\"N_Switchers{pl}_XX\"] = float(((df[\"S_XX\"] != 0) & (~df[\"S_XX\"].isna())).sum())\n",
    "        scalars[f\"N_Stayers{pl}_XX\"] = float((df[\"S_XX\"] == 0).sum())\n",
    "\n",
    "    if ivwaoss == 1:\n",
    "        scalars[f\"N_Switchers_IV{pl}_XX\"] = float(((df[\"SI_XX\"] != 0) & (~df[\"SI_XX\"].isna())).sum())\n",
    "        scalars[f\"N_Stayers_IV{pl}_XX\"] = float((df[\"SI_XX\"] == 0).sum())\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 15) Build polynomial regressors and formula strings\n",
    "    # ------------------------------------------------------------\n",
    "    for pol_level in range(1, order + 1):\n",
    "        df[f\"D1_{pol_level}_XX\"] = df[\"D1_XX\"] ** pol_level\n",
    "\n",
    "    reg_pol_terms = \" + \".join([f\"D1_{k}_XX\" for k in range(1, order + 1)])\n",
    "\n",
    "    if other_treatments:\n",
    "        interact = \"D1_1_XX\"\n",
    "        for v in other_treatments:\n",
    "            interact = f\"{interact} * {v}\"\n",
    "        reg_pol_terms = f\"{reg_pol_terms} + {interact}\"\n",
    "\n",
    "    if ivwaoss == 1:\n",
    "        for pol_level in range(1, order + 1):\n",
    "            df[f\"Z1_{pol_level}_XX\"] = df[\"Z1_XX\"] ** pol_level\n",
    "\n",
    "        IV_reg_pol_terms = \" + \".join([f\"Z1_{k}_XX\" for k in range(1, order + 1)])\n",
    "        if other_treatments:\n",
    "            interact = \"Z1_1_XX\"\n",
    "            for v in other_treatments:\n",
    "                interact = f\"{interact} * {v}\"\n",
    "            IV_reg_pol_terms = f\"{IV_reg_pol_terms} + {interact}\"\n",
    "    else:\n",
    "        IV_reg_pol_terms = \"\"\n",
    "\n",
    "    df[\"S_bis_XX\"] = np.where(df[\"S_XX\"].isna(), np.nan, (df[\"S_XX\"] != 0).astype(float))\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 16) Feasibility check\n",
    "    # ------------------------------------------------------------\n",
    "    if aoss == 1 or waoss == 1:\n",
    "        feasible_est = (gap_XX == 0) and (scalars[f\"N_Switchers{pl}_XX\"] > 0) and (scalars[f\"N_Stayers{pl}_XX\"] > 1)\n",
    "    else:\n",
    "        feasible_est = (gap_XX == 0) and (scalars[f\"N_Switchers_IV{pl}_XX\"] > 0) and (scalars[f\"N_Stayers_IV{pl}_XX\"] > 1)\n",
    "\n",
    "    scalars[f\"P_Ht_{pairwise}{pl}_XX\"] = Mean(\"Ht_XX\", df)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 17) Cluster preparation\n",
    "    # ------------------------------------------------------------\n",
    "    cluster_col = None\n",
    "    if cluster is not None:\n",
    "        if \"cluster_XX\" in df.columns:\n",
    "            cluster_col = \"cluster_XX\"\n",
    "        elif cluster in df.columns:\n",
    "            df[\"cluster_XX\"] = df[cluster]\n",
    "            cluster_col = \"cluster_XX\"\n",
    "        else:\n",
    "            raise ValueError(\"cluster specified but neither cluster_XX nor the given cluster column exists in df.\")\n",
    "        # If cluster is identical to ID, clustering collapses to ID-robust -> treat as no cluster (matches R main behavior).\n",
    "        same_as_id = False\n",
    "        try:\n",
    "            same_as_id = (\n",
    "                df[cluster_col].astype(\"string\").fillna(\"<NA>\")\n",
    "                .equals(df[\"ID_XX\"].astype(\"string\").fillna(\"<NA>\"))\n",
    "            )\n",
    "        except Exception:\n",
    "            same_as_id = False\n",
    "\n",
    "        if same_as_id:\n",
    "            cluster_col = None\n",
    "            cluster = None\n",
    "        else:\n",
    "            # In R main: weight_c_XX = sum(weight_XX) by (cluster, time).\n",
    "            if \"weight_c_XX\" not in df.columns:\n",
    "                if \"weight_XX\" in df.columns:\n",
    "                    _ensure_numeric(df, \"weight_XX\")\n",
    "                    df[\"weight_XX\"] = df[\"weight_XX\"].fillna(0.0)\n",
    "                    df[\"weight_c_XX\"] = df.groupby([cluster_col, \"T_XX\"])[\"weight_XX\"].transform(\"sum\")\n",
    "                else:\n",
    "                    # fallback: unweighted cluster-time counts\n",
    "                    df[\"weight_c_XX\"] = df.groupby([cluster_col, \"T_XX\"])[\"ID_XX\"].transform(\"size\").astype(float)\n",
    "\n",
    "            _ensure_numeric(df, \"weight_c_XX\")\n",
    "            df[\"weight_c_XX\"] = df[\"weight_c_XX\"].fillna(0.0)\n",
    "\n",
    "        if cluster_col is not None:\n",
    "            df[\"_first_in_id\"] = df.groupby(\"ID_XX\").cumcount().eq(0).astype(float)\n",
    "            df[\"_Nc\"] = df.groupby(cluster_col)[\"_first_in_id\"].transform(lambda s: float(np.nansum(s.to_numpy(dtype=float))))\n",
    "            scalars[f\"N_bar_c_{pairwise}{pl}_XX\"] = float(np.nanmean(df[\"_Nc\"].to_numpy(dtype=float)))\n",
    "        df.drop(columns=[\"_first_in_id\", \"_Nc\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # ============================================================\n",
    "    # 18) Estimation if feasible\n",
    "    # ============================================================\n",
    "    if feasible_est:\n",
    "\n",
    "        # -------------------------\n",
    "        # 18A) Common prelims AOSS/WAOSS\n",
    "        # -------------------------\n",
    "        if waoss == 1 or aoss == 1:\n",
    "            df0 = df[df[\"S_XX\"] == 0].copy()\n",
    "\n",
    "            ra_formula = f\"delta_Y_XX ~ {reg_pol_terms}\"\n",
    "            ra_model = smf.wls(ra_formula, data=df0, weights=df0[\"weight_XX\"]).fit()\n",
    "            df = __lpredict(df, \"mean_pred_XX\", ra_model)\n",
    "\n",
    "            df[\"inner_sum_delta_1_2_XX\"] = df[\"delta_Y_XX\"] - df[\"mean_pred_XX\"]\n",
    "            df[\"S0_XX\"] = 1.0 - df[\"S_bis_XX\"]\n",
    "\n",
    "            if not exact_match:\n",
    "                ps0_formula = f\"S0_XX ~ {reg_pol_terms}\"\n",
    "                ps0_model = stata_logit(ps0_formula, df)\n",
    "                df = __lpredict(df, \"PS_0_D_1_XX\", ps0_model)\n",
    "            else:\n",
    "                esbis_formula = f\"S_bis_XX ~ {reg_pol_terms}\"\n",
    "                esbis_model = smf.wls(esbis_formula, data=df, weights=df[\"weight_XX\"]).fit()\n",
    "                df = __lpredict(df, \"ES_bis_XX_D_1\", esbis_model)\n",
    "\n",
    "                es_formula = f\"S_XX ~ {reg_pol_terms}\"\n",
    "                es_model = smf.wls(es_formula, data=df, weights=df[\"weight_XX\"]).fit()\n",
    "                df = __lpredict(df, \"ES_XX_D_1\", es_model)\n",
    "\n",
    "            scalars[f\"PS_0{pl}_XX\"] = Mean(\"S0_XX\", df)\n",
    "\n",
    "        # -------------------------\n",
    "        # 18B) AOSS\n",
    "        # -------------------------\n",
    "        if aoss == 1:\n",
    "            ES = Mean(\"S_bis_XX\", df)\n",
    "            scalars[f\"ES{pl}_XX\"] = ES\n",
    "\n",
    "            scalars[f\"P_{pairwise}{pl}_XX\"] = ES * scalars[f\"P_Ht_{pairwise}{pl}_XX\"]\n",
    "            scalars[f\"PS_sum{pl}_XX\"] = scalars.get(f\"PS_sum{pl}_XX\", 0.0) + scalars[f\"P_{pairwise}{pl}_XX\"]\n",
    "\n",
    "            # Step 1: S_over_delta_D = S_bis / delta_D (for switchers)\n",
    "            df[\"S_over_delta_D_XX\"] = df[\"S_bis_XX\"] / df[\"delta_D_XX\"]\n",
    "            df.loc[df[\"S_bis_XX\"] == 0, \"S_over_delta_D_XX\"] = 0.0\n",
    "\n",
    "            # Step 2: Regress S/deltaD on D1 to get E[S/deltaD | D1]\n",
    "            sdd_formula = f\"S_over_delta_D_XX ~ {reg_pol_terms}\"\n",
    "            sdd_model = smf.wls(sdd_formula, data=df, weights=df[\"weight_XX\"]).fit()\n",
    "            df = __lpredict(df, \"mean_S_over_delta_D_XX\", sdd_model)\n",
    "\n",
    "            # Step 3: Compute delta_1 using DOUBLY-ROBUST formula (matching Stata)\n",
    "            # For switchers (S_bis=1): S_over_deltaD * inner_sum_delta_1_2\n",
    "            # For stayers (S_bis=0): -(mean_S_over_deltaD / PS_0) * inner_sum_delta_1_2\n",
    "            if not exact_match:\n",
    "                df[\"dr_delta1_DR_XX\"] = np.where(\n",
    "                    df[\"S_bis_XX\"] == 0,\n",
    "                    -(df[\"mean_S_over_delta_D_XX\"] / df[\"PS_0_D_1_XX\"].replace(0, np.nan)) * df[\"inner_sum_delta_1_2_XX\"],\n",
    "                    df[\"S_over_delta_D_XX\"] * df[\"inner_sum_delta_1_2_XX\"]\n",
    "                )\n",
    "            else:\n",
    "                denom_exact = (1.0 - df[\"ES_bis_XX_D_1\"]).replace(0, np.nan)\n",
    "                df[\"dr_delta1_DR_XX\"] = np.where(\n",
    "                    df[\"S_bis_XX\"] == 0,\n",
    "                    -(df[\"mean_S_over_delta_D_XX\"] / denom_exact) * df[\"inner_sum_delta_1_2_XX\"],\n",
    "                    df[\"S_over_delta_D_XX\"] * df[\"inner_sum_delta_1_2_XX\"]\n",
    "                )\n",
    "            \n",
    "            # delta_1 = weighted mean of the DR estimator\n",
    "            scalars[f\"delta_1_{pairwise}{pl}_XX\"] = Mean(\"dr_delta1_DR_XX\", df)\n",
    "\n",
    "            # Keep inner_sum_delta_1_XX for compatibility (used elsewhere)\n",
    "            df[\"inner_sum_delta_1_XX\"] = df[\"inner_sum_delta_1_2_XX\"] / df[\"delta_D_XX\"]\n",
    "            df.loc[df[\"delta_D_XX\"] == 0, \"inner_sum_delta_1_XX\"] = np.nan\n",
    "\n",
    "            # Step 4: Influence function (raw_phi) - same as before\n",
    "            if not exact_match:\n",
    "                adj = (1.0 - df[\"S_bis_XX\"]) / df[\"PS_0_D_1_XX\"]\n",
    "                raw_phi = (df[\"S_over_delta_D_XX\"] - df[\"mean_S_over_delta_D_XX\"] * adj) * df[\"inner_sum_delta_1_2_XX\"]\n",
    "            else:\n",
    "                adj = (1.0 - df[\"S_bis_XX\"]) / (1.0 - df[\"ES_bis_XX_D_1\"])\n",
    "                raw_phi = (df[\"S_over_delta_D_XX\"] - df[\"mean_S_over_delta_D_XX\"] * adj) * df[\"inner_sum_delta_1_2_XX\"]\n",
    "\n",
    "            df[f\"Phi_1_{pairwise}{pl}_XX\"] = (\n",
    "                raw_phi - (scalars[f\"delta_1_{pairwise}{pl}_XX\"] * df[\"S_bis_XX\"])\n",
    "            ) / (ES * scalars[f\"P_Ht_{pairwise}{pl}_XX\"])\n",
    "\n",
    "            df.loc[df[\"Ht_XX\"] == 0, f\"Phi_1_{pairwise}{pl}_XX\"] = 0.0\n",
    "\n",
    "            # SE delta_1\n",
    "            if cluster_col is not None:\n",
    "                phi = f\"Phi_1_{pairwise}{pl}_XX\"\n",
    "                df[\"_phi_c\"] = df.groupby(cluster_col)[phi].transform(lambda s: float(np.nansum(s.to_numpy(dtype=float))))\n",
    "                df[\"_first_clus\"] = df.groupby(cluster_col).cumcount().eq(0)\n",
    "                df[\"_phi_c\"] = np.where(df[\"_first_clus\"], df[\"_phi_c\"], np.nan) / scalars[f\"N_bar_c_{pairwise}{pl}_XX\"]\n",
    "\n",
    "                nobs_c = wSum(df[~df[\"_phi_c\"].isna()], w=\"weight_c_XX\")\n",
    "                sd_phi = Sd(\"_phi_c\", df, w=\"weight_c_XX\") / np.sqrt(nobs_c) if nobs_c > 0 else np.nan\n",
    "                scalars[f\"sd_delta_1_{pairwise}{pl}_XX\"] = sd_phi\n",
    "                df.drop(columns=[\"_phi_c\", \"_first_clus\"], inplace=True)\n",
    "            else:\n",
    "                # IMPORTANT: R uses sd() unweighted with ddof=1 here\n",
    "                phi_vals = df[f\"Phi_1_{pairwise}{pl}_XX\"].to_numpy(dtype=float)\n",
    "                scalars[f\"sd_delta_1_{pairwise}{pl}_XX\"] = np.nanstd(phi_vals, ddof=1) / np.sqrt(wSum(df))\n",
    "\n",
    "            se = scalars[f\"sd_delta_1_{pairwise}{pl}_XX\"]\n",
    "            scalars[f\"LB_1_{pairwise}{pl}_XX\"] = scalars[f\"delta_1_{pairwise}{pl}_XX\"] - 1.96 * se\n",
    "            scalars[f\"UB_1_{pairwise}{pl}_XX\"] = scalars[f\"delta_1_{pairwise}{pl}_XX\"] + 1.96 * se\n",
    "\n",
    "            df[f\"S_{pairwise}{pl}_XX\"] = df[\"S_bis_XX\"]\n",
    "            df.loc[df[\"Ht_XX\"] == 0, f\"S_{pairwise}{pl}_XX\"] = 0.0\n",
    "\n",
    "        # -------------------------\n",
    "        # 18C) WAOSS\n",
    "        # -------------------------\n",
    "        if waoss == 1:\n",
    "            scalars[f\"E_abs_delta_D{pl}_XX\"] = Mean(\"abs_delta_D_XX\", df)\n",
    "            scalars[f\"E_abs_delta_D_{pairwise}{pl}_XX\"] = scalars[f\"E_abs_delta_D{pl}_XX\"] * scalars[f\"P_Ht_{pairwise}{pl}_XX\"]\n",
    "            scalars[f\"E_abs_delta_D_sum{pl}_XX\"] = scalars.get(f\"E_abs_delta_D_sum{pl}_XX\", 0.0) + scalars[f\"E_abs_delta_D_{pairwise}{pl}_XX\"]\n",
    "\n",
    "            for suffix in (\"Minus\", \"Plus\"):\n",
    "                target_S = 1.0 if suffix == \"Plus\" else -1.0\n",
    "                df[\"Ster_XX\"] = np.where(df[\"S_XX\"].isna(), np.nan, (df[\"S_XX\"] == target_S).astype(float))\n",
    "\n",
    "                df[\"prod_sgn_delta_D_delta_D_XX\"] = df[\"S_XX\"] * df[\"delta_D_XX\"]\n",
    "                sum_prod = Sum(\"prod_sgn_delta_D_delta_D_XX\", df[df[\"Ster_XX\"] == 1])\n",
    "                scalars[f\"w_{suffix}_{pairwise}{pl}_XX\"] = sum_prod / scalars[f\"N{pl}_XX\"] if scalars[f\"N{pl}_XX\"] > 0 else 0.0\n",
    "\n",
    "                denom = Sum(\"delta_D_XX\", df[df[\"Ster_XX\"] == 1])\n",
    "                scalars[f\"denom_delta_2_{suffix}_{pairwise}{pl}_XX\"] = denom\n",
    "\n",
    "                if estimation_method == \"ra\":\n",
    "                    if denom == 0:\n",
    "                        denom = 1.0\n",
    "                        scalars[f\"denom_delta_2_{suffix}_{pairwise}{pl}_XX\"] = denom\n",
    "                    num = Sum(\"inner_sum_delta_1_2_XX\", df[df[\"Ster_XX\"] == 1])\n",
    "                    scalars[f\"num_delta_2_{suffix}_{pairwise}{pl}_XX\"] = num\n",
    "                    scalars[f\"delta_2_{suffix}_{pairwise}{pl}_XX\"] = num / denom\n",
    "\n",
    "                nb_sw = float(df[df[\"Ster_XX\"] == 1].shape[0])\n",
    "                scalars[f\"nb_Switchers_{suffix}{pl}_XX\"] = nb_sw\n",
    "                scalars[f\"PS_{suffix}1{pl}_XX\"] = nb_sw / scalars[f\"N{pl}_XX\"] if scalars[f\"N{pl}_XX\"] > 0 else 0.0\n",
    "\n",
    "                if not exact_match:\n",
    "                    if scalars[f\"PS_{suffix}1{pl}_XX\"] == 0:\n",
    "                        scalars[f\"delta_2_{suffix}_{pairwise}{pl}_XX\"] = 0.0\n",
    "                        df[f\"PS_1_{suffix}_D_1_XX\"] = 0.0\n",
    "                    else:\n",
    "                        ps1_formula = f\"Ster_XX ~ {reg_pol_terms}\"\n",
    "                        ps1_model = stata_logit(ps1_formula, df)\n",
    "                        df = __lpredict(df, f\"PS_1_{suffix}_D_1_XX\", ps1_model)\n",
    "\n",
    "                        if estimation_method == \"ps\":\n",
    "                            df[f\"delta_Y_P_{suffix}_XX\"] = (\n",
    "                                df[\"delta_Y_XX\"]\n",
    "                                * (df[f\"PS_1_{suffix}_D_1_XX\"] / df[\"PS_0_D_1_XX\"])\n",
    "                                * (scalars[f\"PS_0{pl}_XX\"] / scalars[f\"PS_{suffix}1{pl}_XX\"])\n",
    "                            )\n",
    "                            mean_delta_Y_P = Mean(f\"delta_Y_P_{suffix}_XX\", df[df[\"S_XX\"] == 0])\n",
    "                            mean_delta_Y = Mean(\"delta_Y_XX\", df[df[\"Ster_XX\"] == 1])\n",
    "                            mean_delta_D = Mean(\"delta_D_XX\", df[df[\"Ster_XX\"] == 1])\n",
    "                            scalars[f\"delta_2_{suffix}_{pairwise}{pl}_XX\"] = (mean_delta_Y - mean_delta_Y_P) / mean_delta_D\n",
    "\n",
    "            if estimation_method in (\"ra\", \"ps\"):\n",
    "                w_plus = scalars.get(f\"w_Plus_{pairwise}{pl}_XX\", 0.0)\n",
    "                w_minus = scalars.get(f\"w_Minus_{pairwise}{pl}_XX\", 0.0)\n",
    "                denomw = w_plus + w_minus\n",
    "                scalars[f\"W_Plus_{pairwise}{pl}_XX\"] = (w_plus / denomw) if denomw != 0 else 0.0\n",
    "\n",
    "            if not exact_match:\n",
    "                df[\"dr_delta_Y_XX\"] = (\n",
    "                    (df[\"S_XX\"]\n",
    "                     - ((df.get(\"PS_1_Plus_D_1_XX\", 0.0) - df.get(\"PS_1_Minus_D_1_XX\", 0.0)) / df[\"PS_0_D_1_XX\"])\n",
    "                     * (1.0 - df[\"S_bis_XX\"]))\n",
    "                    * df[\"inner_sum_delta_1_2_XX\"]\n",
    "                )\n",
    "                scalars[f\"denom_dr_delta_2{pl}_XX\"] = Sum(\"dr_delta_Y_XX\", df)\n",
    "\n",
    "            if estimation_method in (\"ra\", \"ps\"):\n",
    "                Wp = scalars[f\"W_Plus_{pairwise}{pl}_XX\"]\n",
    "                scalars[f\"delta_2_{pairwise}{pl}_XX\"] = (\n",
    "                    Wp * scalars[f\"delta_2_Plus_{pairwise}{pl}_XX\"]\n",
    "                    + (1.0 - Wp) * scalars[f\"delta_2_Minus_{pairwise}{pl}_XX\"]\n",
    "                )\n",
    "            elif estimation_method == \"dr\":\n",
    "                sum_abs = Sum(\"abs_delta_D_XX\", df)\n",
    "                scalars[f\"delta_2_{pairwise}{pl}_XX\"] = scalars[f\"denom_dr_delta_2{pl}_XX\"] / sum_abs if sum_abs != 0 else 0.0\n",
    "\n",
    "            if not exact_match:\n",
    "                df[f\"Phi_2_{pairwise}{pl}_XX\"] = df[\"dr_delta_Y_XX\"] - scalars[f\"delta_2_{pairwise}{pl}_XX\"] * df[\"abs_delta_D_XX\"]\n",
    "            else:\n",
    "                df[f\"Phi_2_{pairwise}{pl}_XX\"] = (\n",
    "                    (df[\"S_XX\"] - df[\"ES_XX_D_1\"] * ((1.0 - df[\"S_bis_XX\"]) / (1.0 - df[\"ES_bis_XX_D_1\"])))\n",
    "                    * df[\"inner_sum_delta_1_2_XX\"]\n",
    "                    - scalars[f\"delta_2_{pairwise}{pl}_XX\"] * df[\"abs_delta_D_XX\"]\n",
    "                )\n",
    "\n",
    "            denom_if = scalars[f\"P_Ht_{pairwise}{pl}_XX\"] * scalars[f\"E_abs_delta_D{pl}_XX\"]\n",
    "            df[f\"Phi_2_{pairwise}{pl}_XX\"] = df[f\"Phi_2_{pairwise}{pl}_XX\"] / denom_if if denom_if != 0 else np.nan\n",
    "            df.loc[df[\"Ht_XX\"] == 0, f\"Phi_2_{pairwise}{pl}_XX\"] = 0.0\n",
    "\n",
    "            # SE delta_2\n",
    "            if cluster_col is not None:\n",
    "                phi = f\"Phi_2_{pairwise}{pl}_XX\"\n",
    "                df[\"_phi_c\"] = df.groupby(cluster_col)[phi].transform(lambda s: float(np.nansum(s.to_numpy(dtype=float))))\n",
    "                df[\"_first_clus\"] = df.groupby(cluster_col).cumcount().eq(0)\n",
    "                df[\"_phi_c\"] = np.where(df[\"_first_clus\"], df[\"_phi_c\"], np.nan) / scalars[f\"N_bar_c_{pairwise}{pl}_XX\"]\n",
    "\n",
    "                nobs_c = wSum(df[~df[\"_phi_c\"].isna()], w=\"weight_c_XX\")\n",
    "                sd_phi = Sd(\"_phi_c\", df, w=\"weight_c_XX\") / np.sqrt(nobs_c) if nobs_c > 0 else np.nan\n",
    "                scalars[f\"sd_delta_2_{pairwise}{pl}_XX\"] = sd_phi\n",
    "                df.drop(columns=[\"_phi_c\", \"_first_clus\"], inplace=True)\n",
    "            else:\n",
    "                # R uses its weighted Sd() helper here (variance divided by sum of weights).\n",
    "                scalars[f\"sd_delta_2_{pairwise}{pl}_XX\"] = Sd(\n",
    "                    f\"Phi_2_{pairwise}{pl}_XX\", df\n",
    "                ) / np.sqrt(wSum(df))\n",
    "\n",
    "            se = scalars[f\"sd_delta_2_{pairwise}{pl}_XX\"]\n",
    "            scalars[f\"LB_2_{pairwise}{pl}_XX\"] = scalars[f\"delta_2_{pairwise}{pl}_XX\"] - 1.96 * se\n",
    "            scalars[f\"UB_2_{pairwise}{pl}_XX\"] = scalars[f\"delta_2_{pairwise}{pl}_XX\"] + 1.96 * se\n",
    "\n",
    "            df[f\"abs_delta_D_{pairwise}{pl}_XX\"] = np.where(df[\"Ht_XX\"] == 0, 0.0, df[\"abs_delta_D_XX\"])\n",
    "\n",
    "        # -------------------------\n",
    "        # 18D) IV-WAOSS (delta_3)\n",
    "        # -------------------------\n",
    "        if ivwaoss == 1:\n",
    "            scalars[f\"E_abs_delta_Z{pl}_XX\"] = Mean(\"abs_delta_Z_XX\", df)\n",
    "\n",
    "            df[\"SI_bis_XX\"] = ((df[\"SI_XX\"] != 0) & (~df[\"SI_XX\"].isna())).astype(float)\n",
    "            df[\"SI_Plus_XX\"] = np.where(df[\"SI_XX\"].isna(), np.nan, (df[\"SI_XX\"] == 1).astype(float))\n",
    "            df[\"SI_Minus_XX\"] = np.where(df[\"SI_XX\"].isna(), np.nan, (df[\"SI_XX\"] == -1).astype(float))\n",
    "\n",
    "            df[\"S_IV_0_XX\"] = 1.0 - df[\"SI_bis_XX\"]\n",
    "\n",
    "            if not exact_match:\n",
    "                psiv0_formula = f\"S_IV_0_XX ~ {IV_reg_pol_terms}\"\n",
    "                psiv0_model = stata_logit(psiv0_formula, df)\n",
    "                df = __lpredict(df, \"PS_IV_0_Z_1_XX\", psiv0_model)\n",
    "            else:\n",
    "                esibis_formula = f\"SI_bis_XX ~ {IV_reg_pol_terms}\"\n",
    "                esibis_model = smf.wls(esibis_formula, data=df, weights=df[\"weight_XX\"]).fit()\n",
    "                df = __lpredict(df, \"ES_I_bis_XX_Z_1\", esibis_model)\n",
    "\n",
    "                esi_formula = f\"SI_XX ~ {IV_reg_pol_terms}\"\n",
    "                esi_model = smf.wls(esi_formula, data=df, weights=df[\"weight_XX\"]).fit()\n",
    "                df = __lpredict(df, \"ES_I_XX_Z_1\", esi_model)\n",
    "\n",
    "            scalars[f\"PS_IV_0{pl}_XX\"] = Mean(\"S_IV_0_XX\", df)\n",
    "\n",
    "            for suffix in (\"Minus\", \"Plus\"):\n",
    "                flag = \"SI_Minus_XX\" if suffix == \"Minus\" else \"SI_Plus_XX\"\n",
    "                nb = float((df[flag] == 1).sum())\n",
    "                scalars[f\"nb_Switchers_I_{suffix}{pl}_XX\"] = nb\n",
    "                scalars[f\"PS_I_{suffix}_1{pl}_XX\"] = nb / scalars[f\"N{pl}_XX\"] if scalars[f\"N{pl}_XX\"] > 0 else 0.0\n",
    "\n",
    "                if scalars[f\"PS_I_{suffix}_1{pl}_XX\"] == 0:\n",
    "                    df[f\"PS_I_{suffix}_1_Z_1_XX\"] = 0.0\n",
    "                else:\n",
    "                    if not exact_match:\n",
    "                        psis_formula = f\"{flag} ~ {IV_reg_pol_terms}\"\n",
    "                        psis_model = stata_logit(psis_formula, df)\n",
    "                        df = __lpredict(df, f\"PS_I_{suffix}_1_Z_1_XX\", psis_model)\n",
    "\n",
    "            \n",
    "            # Products used by the PS estimator (mirrors the R code)\n",
    "            df[\"prod_sgn_delta_Z_delta_Y_XX\"] = df[\"SI_XX\"] * df[\"delta_Y_XX\"]\n",
    "            df[\"prod_sgn_delta_Z_delta_D_XX\"] = df[\"SI_XX\"] * df[\"delta_D_XX\"]\n",
    "\n",
    "            df_temp = df[df[\"SI_XX\"] == 0].copy()\n",
    "            mY_formula = f\"delta_Y_XX ~ {IV_reg_pol_terms}\"\n",
    "            mY_model = smf.wls(mY_formula, data=df_temp, weights=df_temp[\"weight_XX\"]).fit()\n",
    "            df = __lpredict(df, \"mean_delta_Y_pred_IV_XX\", mY_model)\n",
    "            df[\"inner_sum_IV_num_XX\"] = df[\"delta_Y_XX\"] - df[\"mean_delta_Y_pred_IV_XX\"]\n",
    "\n",
    "            mD_formula = f\"delta_D_XX ~ {IV_reg_pol_terms}\"\n",
    "            mD_model = smf.wls(mD_formula, data=df_temp, weights=df_temp[\"weight_XX\"]).fit()\n",
    "            df = __lpredict(df, \"mean_delta_D_pred_IV_XX\", mD_model)\n",
    "            df[\"inner_sum_IV_denom_XX\"] = df[\"delta_D_XX\"] - df[\"mean_delta_D_pred_IV_XX\"]\n",
    "\n",
    "            if estimation_method == \"ra\":\n",
    "                # Multiply by SI_XX \n",
    "                df[\"inner_sum_IV_num_XX\"] = df[\"inner_sum_IV_num_XX\"] * df[\"SI_XX\"]\n",
    "                df[\"inner_sum_IV_denom_XX\"] = df[\"inner_sum_IV_denom_XX\"] * df[\"SI_XX\"]\n",
    "                \n",
    "                # FIX: Use Sum/N instead of Mean to match Stata behavior\n",
    "                # Mean can give different results if there are NA values\n",
    "                N_total = float(scalars[f\"N{pl}_XX\"])\n",
    "                if N_total > 0:\n",
    "                    scalars[f\"num_delta_IV_{pairwise}{pl}_XX\"] = Sum(\"inner_sum_IV_num_XX\", df) / N_total\n",
    "                    scalars[f\"denom_delta_IV_{pairwise}{pl}_XX\"] = Sum(\"inner_sum_IV_denom_XX\", df) / N_total\n",
    "                else:\n",
    "                    scalars[f\"num_delta_IV_{pairwise}{pl}_XX\"] = np.nan\n",
    "                    scalars[f\"denom_delta_IV_{pairwise}{pl}_XX\"] = np.nan\n",
    "\n",
    "            if estimation_method == \"ps\":\n",
    "                if exact_match:\n",
    "                    raise ValueError(\"estimation_method='ps' is not implemented with exact_match=True (matches R behavior).\")\n",
    "\n",
    "                # Reweight stayers (SI_bis_XX==0) to mimic Stata's PS reweighting in the R code\n",
    "                df[\"delta_Y_P_IV_XX\"] = (\n",
    "                    df[\"delta_Y_XX\"]\n",
    "                    * ((df.get(\"PS_I_Plus_1_Z_1_XX\", 0.0) - df.get(\"PS_I_Minus_1_Z_1_XX\", 0.0)) / df[\"PS_IV_0_Z_1_XX\"])\n",
    "                    * scalars[f\"PS_IV_0{pl}_XX\"]\n",
    "                )\n",
    "                mean_delta_Y_P_IV = Mean(\"delta_Y_P_IV_XX\", df[df[\"SI_bis_XX\"] == 0])\n",
    "                mean_prod_sgn_Z_delta_Y = Mean(\"prod_sgn_delta_Z_delta_Y_XX\", df)\n",
    "                scalars[f\"num_delta_IV_{pairwise}{pl}_XX\"] = mean_prod_sgn_Z_delta_Y - mean_delta_Y_P_IV\n",
    "\n",
    "                df[\"delta_D_P_IV_XX\"] = (\n",
    "                    df[\"delta_D_XX\"]\n",
    "                    * ((df.get(\"PS_I_Plus_1_Z_1_XX\", 0.0) - df.get(\"PS_I_Minus_1_Z_1_XX\", 0.0)) / df[\"PS_IV_0_Z_1_XX\"])\n",
    "                    * scalars[f\"PS_IV_0{pl}_XX\"]\n",
    "                )\n",
    "                mean_delta_D_P_IV = Mean(\"delta_D_P_IV_XX\", df[df[\"SI_bis_XX\"] == 0])\n",
    "                mean_prod_sgn_Z_delta_D = Mean(\"prod_sgn_delta_Z_delta_D_XX\", df)\n",
    "                scalars[f\"denom_delta_IV_{pairwise}{pl}_XX\"] = mean_prod_sgn_Z_delta_D - mean_delta_D_P_IV\n",
    "\n",
    "            if estimation_method == \"dr\":\n",
    "                df[\"dr_IV_delta_Y_XX\"] = (\n",
    "                    (df[\"SI_XX\"] - ((df.get(\"PS_I_Plus_1_Z_1_XX\", 0.0) - df.get(\"PS_I_Minus_1_Z_1_XX\", 0.0)) / df[\"PS_IV_0_Z_1_XX\"])\n",
    "                     * (1.0 - df[\"SI_bis_XX\"]))\n",
    "                    * df[\"inner_sum_IV_num_XX\"]\n",
    "                )\n",
    "                scalars[f\"num_delta_IV_{pairwise}{pl}_XX\"] = Mean(\"dr_IV_delta_Y_XX\", df)\n",
    "\n",
    "                df[\"dr_IV_delta_D_XX\"] = (\n",
    "                    (df[\"SI_XX\"] - ((df.get(\"PS_I_Plus_1_Z_1_XX\", 0.0) - df.get(\"PS_I_Minus_1_Z_1_XX\", 0.0)) / df[\"PS_IV_0_Z_1_XX\"])\n",
    "                     * (1.0 - df[\"SI_bis_XX\"]))\n",
    "                    * df[\"inner_sum_IV_denom_XX\"]\n",
    "                )\n",
    "                scalars[f\"denom_delta_IV_{pairwise}{pl}_XX\"] = Mean(\"dr_IV_delta_D_XX\", df)\n",
    "\n",
    "            scalars[f\"delta_3_{pairwise}{pl}_XX\"] = (\n",
    "                scalars[f\"num_delta_IV_{pairwise}{pl}_XX\"] / scalars[f\"denom_delta_IV_{pairwise}{pl}_XX\"]\n",
    "                if scalars[f\"denom_delta_IV_{pairwise}{pl}_XX\"] != 0 else np.nan\n",
    "            )\n",
    "\n",
    "            \n",
    "            # ------------------------------------------------------------\n",
    "            # Influence function for IV-WAOSS (delta_3) + its pairwise SE\n",
    "            # Mirrors the R code:\n",
    "            #   Phi_Y_XX, Phi_D_XX  ->  Phi_3 = (Phi_Y - delta_3 * Phi_D) / delta_Dbar\n",
    "            # ------------------------------------------------------------\n",
    "\n",
    "            # Track the overall weight used later in did_multiplegt_stat_main\n",
    "            scalars[f\"denom_delta_IV_sum{pl}_XX\"] = (\n",
    "                scalars.get(f\"denom_delta_IV_sum{pl}_XX\", 0.0)\n",
    "                + scalars.get(f\"denom_delta_IV_{pairwise}{pl}_XX\", np.nan)\n",
    "            )\n",
    "\n",
    "            # \"Moment means\" (used to center Phi_Y/Phi_D)\n",
    "            scalars[f\"delta_Y{pl}_XX\"] = Mean(\"inner_sum_IV_num_XX\", df)\n",
    "            scalars[f\"delta_D{pl}_XX\"] = Mean(\"inner_sum_IV_denom_XX\", df)\n",
    "\n",
    "            # Residuals from the \"stayers\" regressions\n",
    "            df[\"resid_Y_IV_XX\"] = df[\"delta_Y_XX\"] - df[\"mean_delta_Y_pred_IV_XX\"]\n",
    "            df[\"resid_D_IV_XX\"] = df[\"delta_D_XX\"] - df[\"mean_delta_D_pred_IV_XX\"]\n",
    "\n",
    "            E_abs = scalars.get(f\"E_abs_delta_Z{pl}_XX\", np.nan)\n",
    "\n",
    "            # DR-style \"score\" on the residuals\n",
    "            if not exact_match:\n",
    "                denom_ps = df[\"PS_IV_0_Z_1_XX\"].replace({0.0: np.nan})\n",
    "                score = (\n",
    "                    df[\"SI_XX\"]\n",
    "                    - (df.get(\"PS_I_Plus_1_Z_1_XX\", 0.0) - df.get(\"PS_I_Minus_1_Z_1_XX\", 0.0))\n",
    "                    * (1.0 - df[\"SI_bis_XX\"]) / denom_ps\n",
    "                )\n",
    "            else:\n",
    "                denom_es = (1.0 - df[\"ES_I_bis_XX_Z_1\"]).replace({0.0: np.nan})\n",
    "                score = (\n",
    "                    df[\"SI_XX\"]\n",
    "                    - df[\"ES_I_XX_Z_1\"] * ((1.0 - df[\"SI_bis_XX\"]) / denom_es)\n",
    "                )\n",
    "\n",
    "            df[\"Phi_Y_XX\"] = (\n",
    "                score * df[\"resid_Y_IV_XX\"] - scalars[f\"delta_Y{pl}_XX\"] * df[\"abs_delta_Z_XX\"]\n",
    "            ) / E_abs\n",
    "\n",
    "            df[\"Phi_D_XX\"] = (\n",
    "                score * df[\"resid_D_IV_XX\"] - scalars[f\"delta_D{pl}_XX\"] * df[\"abs_delta_Z_XX\"]\n",
    "            ) / E_abs\n",
    "\n",
    "            delta_D_bar = scalars.get(f\"delta_D{pl}_XX\", np.nan)\n",
    "            delta3 = scalars.get(f\"delta_3_{pairwise}{pl}_XX\", np.nan)\n",
    "\n",
    "            if (delta_D_bar is None) or np.isnan(delta_D_bar) or (delta_D_bar == 0):\n",
    "                df[f\"Phi_3_{pairwise}{pl}_XX\"] = np.nan\n",
    "                scalars[f\"sd_delta_3_{pairwise}{pl}_XX\"] = np.nan\n",
    "                scalars[f\"LB_3_{pairwise}{pl}_XX\"] = np.nan\n",
    "                scalars[f\"UB_3_{pairwise}{pl}_XX\"] = np.nan\n",
    "            else:\n",
    "                df[f\"Phi_3_{pairwise}{pl}_XX\"] = (df[\"Phi_Y_XX\"] - delta3 * df[\"Phi_D_XX\"]) / delta_D_bar\n",
    "\n",
    "                # SE for the pairwise delta_3\n",
    "                if cluster_col is not None:\n",
    "                    phi = f\"Phi_3_{pairwise}{pl}_XX\"\n",
    "                    df[\"_phi_c\"] = df.groupby(cluster_col)[phi].transform(\n",
    "                        lambda s: float(np.nansum(s.to_numpy(dtype=float)))\n",
    "                    )\n",
    "                    df[\"_first_clus\"] = df.groupby(cluster_col).cumcount().eq(0)\n",
    "                    df[\"_phi_c\"] = np.where(df[\"_first_clus\"], df[\"_phi_c\"], np.nan) / scalars[\n",
    "                        f\"N_bar_c_{pairwise}{pl}_XX\"\n",
    "                    ]\n",
    "\n",
    "                    nobs_c = wSum(df[~df[\"_phi_c\"].isna()], w=\"weight_c_XX\")\n",
    "                    sd_phi = Sd(\"_phi_c\", df, w=\"weight_c_XX\") / np.sqrt(nobs_c) if nobs_c > 0 else np.nan\n",
    "                    scalars[f\"sd_delta_3_{pairwise}{pl}_XX\"] = sd_phi\n",
    "                    df.drop(columns=[\"_phi_c\", \"_first_clus\"], inplace=True)\n",
    "                else:\n",
    "                    # R uses its weighted Sd() helper here too.\n",
    "                    scalars[f\"sd_delta_3_{pairwise}{pl}_XX\"] = Sd(\n",
    "                        f\"Phi_3_{pairwise}{pl}_XX\", df\n",
    "                    ) / np.sqrt(wSum(df))\n",
    "\n",
    "                se3 = scalars[f\"sd_delta_3_{pairwise}{pl}_XX\"]\n",
    "                scalars[f\"LB_3_{pairwise}{pl}_XX\"] = delta3 - 1.96 * se3\n",
    "                scalars[f\"UB_3_{pairwise}{pl}_XX\"] = delta3 + 1.96 * se3\n",
    "\n",
    "            # This is later used in did_multiplegt_stat_main for weighting across pairs\n",
    "            df[f\"inner_sum_IV_denom_{pairwise}{pl}_XX\"] = df[\"inner_sum_IV_denom_XX\"]\n",
    "\n",
    "        scalars[f\"non_missing_{pairwise}{pl}_XX\"] = 1.0\n",
    "\n",
    "    else:\n",
    "        # Not feasible => defaults\n",
    "        for i in (1, 2, 3):\n",
    "            scalars[f\"delta_{i}_{pairwise}{pl}_XX\"] = 0.0\n",
    "            scalars[f\"sd_delta_{i}_{pairwise}{pl}_XX\"] = np.nan\n",
    "            scalars[f\"LB_{i}_{pairwise}{pl}_XX\"] = np.nan\n",
    "            scalars[f\"UB_{i}_{pairwise}{pl}_XX\"] = np.nan\n",
    "            df[f\"Phi_{i}_{pairwise}{pl}_XX\"] = np.nan\n",
    "\n",
    "        if aoss == 1:\n",
    "            scalars[f\"P_{pairwise}{pl}_XX\"] = 0.0\n",
    "        if waoss == 1:\n",
    "            scalars[f\"E_abs_delta_D_{pairwise}{pl}_XX\"] = 0.0\n",
    "        if ivwaoss == 1:\n",
    "            scalars[f\"denom_delta_IV_{pairwise}{pl}_XX\"] = 0.0\n",
    "\n",
    "        scalars[f\"non_missing_{pairwise}{pl}_XX\"] = 0.0\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 20) Prepare \"to_add\" DataFrame\n",
    "    # ------------------------------------------------------------\n",
    "    df = df.sort_values([\"ID_XX\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    keep_cols = [\n",
    "        \"ID_XX\",\n",
    "        f\"Phi_1_{pairwise}{pl}_XX\",\n",
    "        f\"Phi_2_{pairwise}{pl}_XX\",\n",
    "        f\"Phi_3_{pairwise}{pl}_XX\",\n",
    "        f\"S_{pairwise}{pl}_XX\",\n",
    "        f\"abs_delta_D_{pairwise}{pl}_XX\",\n",
    "        f\"used_in_{pairwise}{pl}_XX\",\n",
    "        f\"inner_sum_IV_denom_{pairwise}{pl}_XX\",\n",
    "    ]\n",
    "\n",
    "    if cluster is not None:\n",
    "        if \"cluster_XX\" in df.columns:\n",
    "            keep_cols.append(\"cluster_XX\")\n",
    "        elif cluster in df.columns:\n",
    "            keep_cols.append(cluster)\n",
    "\n",
    "    keep_cols = [c for c in keep_cols if c in df.columns]\n",
    "    out_df = df.loc[:, keep_cols].copy()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 21) Final scalar bookkeeping for aggregation\n",
    "    # ------------------------------------------------------------\n",
    "    if waoss == 1 or aoss == 1:\n",
    "        scalars[f\"N_Switchers_1_{pairwise}{pl}_XX\"] = scalars.get(f\"N_Switchers{pl}_XX\", 0.0)\n",
    "        scalars[f\"N_Stayers_1_{pairwise}{pl}_XX\"] = scalars.get(f\"N_Stayers{pl}_XX\", 0.0)\n",
    "        scalars[f\"N_Switchers_2_{pairwise}{pl}_XX\"] = scalars.get(f\"N_Switchers{pl}_XX\", 0.0)\n",
    "        scalars[f\"N_Stayers_2_{pairwise}{pl}_XX\"] = scalars.get(f\"N_Stayers{pl}_XX\", 0.0)\n",
    "\n",
    "    if ivwaoss == 1:\n",
    "        scalars[f\"N_Switchers_3_{pairwise}{pl}_XX\"] = scalars.get(f\"N_Switchers_IV{pl}_XX\", 0.0)\n",
    "        scalars[f\"N_Stayers_3_{pairwise}{pl}_XX\"] = scalars.get(f\"N_Stayers_IV{pl}_XX\", 0.0)\n",
    "\n",
    "    for i, active in enumerate([aoss, waoss, ivwaoss], start=1):\n",
    "        if active == 1:\n",
    "            scalars[f\"delta_{i}_{pairwise}{pl}_XX\"] = scalars.get(f\"delta_{i}_{pairwise}{pl}_XX\", 0.0)\n",
    "            scalars[f\"sd_delta_{i}_{pairwise}{pl}_XX\"] = scalars.get(f\"sd_delta_{i}_{pairwise}{pl}_XX\", np.nan)\n",
    "            scalars[f\"LB_{i}_{pairwise}{pl}_XX\"] = scalars.get(f\"LB_{i}_{pairwise}{pl}_XX\", np.nan)\n",
    "            scalars[f\"UB_{i}_{pairwise}{pl}_XX\"] = scalars.get(f\"UB_{i}_{pairwise}{pl}_XX\", np.nan)\n",
    "\n",
    "    return {\"scalars\": scalars, \"to_add\": out_df}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ra_task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
